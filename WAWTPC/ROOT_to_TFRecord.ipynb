{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workspace setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:48:11.366836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from os.path import isfile\n",
    "import io_functions as io\n",
    "\n",
    "\n",
    "from tensorflow.data import Dataset, TFRecordDataset\n",
    "from tensorflow.io import TFRecordWriter, TFRecordOptions\n",
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Example, Features, Feature\n",
    "\n",
    "from tensorflow.python.ops.numpy_ops import np_config\n",
    "np_config.enable_numpy_behavior()\n",
    "dataPath = '/scratch/pszyc'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecord creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def serialize(charge_array, target):\n",
    "  feature = {'myChargeArray' : _bytes_feature(tf.io.serialize_tensor(charge_array)),\n",
    "             'target' : _bytes_feature(tf.io.serialize_tensor(target))}\n",
    "  example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example.SerializeToString()\n",
    "\n",
    "def conversion(filename, queue):\n",
    "    options = TFRecordOptions(compression_type='GZIP')\n",
    "    writer = TFRecordWriter(filename, options=options)\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item == None:\n",
    "            break\n",
    "        charge_array, target = item\n",
    "        charge_array= io.proc_features(charge_array)\n",
    "        \n",
    "        example = serialize(charge_array, target)\n",
    "        writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XYZtoUVWT(data):\n",
    "    referencePoint = tf.constant([-138.9971, 98.25])\n",
    "    phi = np.pi/6.0\n",
    "    stripPitch = 1.5\n",
    "    f = 1.0/25*6.46\n",
    "    u = -(data[:, 1]-99.75)\n",
    "    v = (data[:, 0]-referencePoint[0]) * np.cos(phi) - (data[:, 1]-referencePoint[1]) * np.sin(phi)\n",
    "    w = (data[:, 0]-referencePoint[0]) * np.cos(-phi) - (data[:, 1]-referencePoint[1]) * np.sin(-phi) + 98.75\n",
    "    t = data[:, 2]/f + 256\n",
    "    u/=stripPitch\n",
    "    v/=stripPitch\n",
    "    w/=stripPitch\n",
    "    return tf.stack([u,v,w,t], axis=0).T\n",
    "\n",
    "def conversion_uvwt(filename, queue):\n",
    "    options = TFRecordOptions(compression_type='GZIP')\n",
    "    writer = TFRecordWriter(filename, options=options)\n",
    "    scale = 100\n",
    "    n_projections = 3\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item == None:\n",
    "            break\n",
    "        myChargeArray, target = item\n",
    "        charge_array= io.proc_features(myChargeArray)\n",
    "        charge_array = tf.transpose(charge_array, perm = [0, 3, 1, 2])\n",
    "        uvwt_1 = XYZtoUVWT(scale*target[:, 0:3])\n",
    "        uvwt_2 = XYZtoUVWT(scale*target[:, 3:6])\n",
    "        uvwt_3 = XYZtoUVWT(scale*target[:, 6:9])\n",
    "\n",
    "        points = []\n",
    "        for i in range(n_projections):\n",
    "          points.append([\n",
    "              uvwt_1[:, 3], uvwt_1[:, i],\n",
    "              uvwt_2[:, 3], uvwt_2[:, i],\n",
    "              uvwt_3[:, 3], uvwt_3[:, i]\n",
    "          ])\n",
    "        points = np.stack(points, axis = 1).T\n",
    "        for index in range(points.shape[0]):\n",
    "          example = serialize(charge_array[index], points[index])\n",
    "          writer.write(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save(output_files, datasetGenerator, conversion_function):\n",
    "    nFiles = len(output_files)\n",
    "                 \n",
    "    for file in output_files:\n",
    "        if isfile(file):\n",
    "            raise Exception('output file already exists')\n",
    "    \n",
    "    if __name__ == '__main__':\n",
    "        processes = []\n",
    "        q = Queue(2*nFiles)\n",
    "\n",
    "        for name in output_files:\n",
    "            p = Process(target=conversion_function, args=(name, q))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "            print(p.name + ' started')\n",
    "    \n",
    "        counter = 0\n",
    "        for item in datasetGenerator:\n",
    "            q.put(item)\n",
    "            counter+=1\n",
    "            if counter%100 == 0:\n",
    "                print(f'read {counter} batches')\n",
    "    \n",
    "        for _ in range(nFiles):\n",
    "            q.put(None)\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            print(p.name + ' done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"out_random_sigma-001\"\n",
    "batchSize = 5\n",
    "nFiles = 5 # number of output files, equal to number of processes\n",
    "\n",
    "input_files = [f'{dataPath}/{file}.root:TPCData']\n",
    "output_files = [f\"{dataPath}/data/{file}-part-{i}.tfrecord\" for i in range(nFiles)]\n",
    "datasetGenerator = io.minimal_generator(files=input_files, batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1 started\n",
      "Process-2 started\n",
      "Process-3 started\n",
      "Process-4 started\n",
      "Process-5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:03:28.324323: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:03:28.480230: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:03:28.596708: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:03:28.784603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:03:28.899872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 100 batches\n",
      "read 200 batches\n",
      "read 300 batches\n",
      "read 400 batches\n",
      "read 500 batches\n",
      "read 600 batches\n",
      "read 700 batches\n",
      "read 800 batches\n",
      "read 900 batches\n",
      "read 1000 batches\n",
      "read 1100 batches\n",
      "read 1200 batches\n",
      "read 1300 batches\n",
      "read 1400 batches\n",
      "read 1500 batches\n",
      "read 1600 batches\n",
      "read 1700 batches\n",
      "read 1800 batches\n",
      "read 1900 batches\n",
      "read 2000 batches\n",
      "read 2100 batches\n",
      "read 2200 batches\n",
      "read 2300 batches\n",
      "read 2400 batches\n",
      "read 2500 batches\n",
      "read 2600 batches\n",
      "read 2700 batches\n",
      "read 2800 batches\n",
      "read 2900 batches\n",
      "read 3000 batches\n",
      "read 3100 batches\n",
      "read 3200 batches\n",
      "read 3300 batches\n",
      "read 3400 batches\n",
      "read 3500 batches\n",
      "read 3600 batches\n",
      "read 3700 batches\n",
      "read 3800 batches\n",
      "read 3900 batches\n",
      "read 4000 batches\n",
      "read 4100 batches\n",
      "read 4200 batches\n",
      "read 4300 batches\n",
      "read 4400 batches\n",
      "read 4500 batches\n",
      "read 4600 batches\n",
      "read 4700 batches\n",
      "read 4800 batches\n",
      "read 4900 batches\n",
      "read 5000 batches\n",
      "read 5100 batches\n",
      "read 5200 batches\n",
      "read 5300 batches\n",
      "read 5400 batches\n",
      "read 5500 batches\n",
      "read 5600 batches\n",
      "read 5700 batches\n",
      "read 5800 batches\n",
      "read 5900 batches\n",
      "read 6000 batches\n",
      "read 6100 batches\n",
      "read 6200 batches\n",
      "read 6300 batches\n",
      "read 6400 batches\n",
      "read 6500 batches\n",
      "read 6600 batches\n",
      "read 6700 batches\n",
      "read 6800 batches\n",
      "read 6900 batches\n",
      "read 7000 batches\n",
      "read 7100 batches\n",
      "read 7200 batches\n",
      "read 7300 batches\n",
      "read 7400 batches\n",
      "read 7500 batches\n",
      "read 7600 batches\n",
      "read 7700 batches\n",
      "read 7800 batches\n",
      "read 7900 batches\n",
      "read 8000 batches\n",
      "read 8100 batches\n",
      "read 8200 batches\n",
      "read 8300 batches\n",
      "read 8400 batches\n",
      "read 8500 batches\n",
      "read 8600 batches\n",
      "read 8700 batches\n",
      "read 8800 batches\n",
      "read 8900 batches\n",
      "read 9000 batches\n",
      "read 9100 batches\n",
      "read 9200 batches\n",
      "read 9300 batches\n",
      "read 9400 batches\n",
      "read 9500 batches\n",
      "read 9600 batches\n",
      "read 9700 batches\n",
      "read 9800 batches\n",
      "read 9900 batches\n",
      "read 10000 batches\n",
      "read 10100 batches\n",
      "read 10200 batches\n",
      "read 10300 batches\n",
      "read 10400 batches\n",
      "read 10500 batches\n",
      "read 10600 batches\n",
      "read 10700 batches\n",
      "read 10800 batches\n",
      "read 10900 batches\n",
      "read 11000 batches\n",
      "read 11100 batches\n",
      "read 11200 batches\n",
      "read 11300 batches\n",
      "read 11400 batches\n",
      "read 11500 batches\n",
      "read 11600 batches\n",
      "read 11700 batches\n",
      "read 11800 batches\n",
      "read 11900 batches\n",
      "read 12000 batches\n",
      "read 12100 batches\n",
      "read 12200 batches\n",
      "read 12300 batches\n",
      "read 12400 batches\n",
      "read 12500 batches\n",
      "read 12600 batches\n",
      "read 12700 batches\n",
      "read 12800 batches\n",
      "read 12900 batches\n",
      "read 13000 batches\n",
      "read 13100 batches\n",
      "read 13200 batches\n",
      "read 13300 batches\n",
      "read 13400 batches\n",
      "read 13500 batches\n",
      "read 13600 batches\n",
      "read 13700 batches\n",
      "read 13800 batches\n",
      "read 13900 batches\n",
      "read 14000 batches\n",
      "read 14100 batches\n",
      "read 14200 batches\n",
      "read 14300 batches\n",
      "read 14400 batches\n",
      "read 14500 batches\n",
      "read 14600 batches\n",
      "read 14700 batches\n",
      "read 14800 batches\n",
      "read 14900 batches\n",
      "read 15000 batches\n",
      "read 15100 batches\n",
      "read 15200 batches\n",
      "read 15300 batches\n",
      "read 15400 batches\n",
      "read 15500 batches\n",
      "read 15600 batches\n",
      "read 15700 batches\n",
      "read 15800 batches\n",
      "read 15900 batches\n",
      "read 16000 batches\n",
      "read 16100 batches\n",
      "read 16200 batches\n",
      "read 16300 batches\n",
      "read 16400 batches\n",
      "read 16500 batches\n",
      "read 16600 batches\n",
      "read 16700 batches\n",
      "read 16800 batches\n",
      "read 16900 batches\n",
      "read 17000 batches\n",
      "read 17100 batches\n",
      "read 17200 batches\n",
      "read 17300 batches\n",
      "read 17400 batches\n",
      "read 17500 batches\n",
      "read 17600 batches\n",
      "read 17700 batches\n",
      "read 17800 batches\n",
      "read 17900 batches\n",
      "read 18000 batches\n",
      "read 18100 batches\n",
      "read 18200 batches\n",
      "read 18300 batches\n",
      "read 18400 batches\n",
      "read 18500 batches\n",
      "read 18600 batches\n",
      "read 18700 batches\n",
      "read 18800 batches\n",
      "read 18900 batches\n",
      "read 19000 batches\n",
      "read 19100 batches\n",
      "read 19200 batches\n",
      "read 19300 batches\n",
      "read 19400 batches\n",
      "read 19500 batches\n",
      "read 19600 batches\n",
      "read 19700 batches\n",
      "read 19800 batches\n",
      "read 19900 batches\n",
      "read 20000 batches\n",
      "Process-1 done\n",
      "Process-2 done\n",
      "Process-3 done\n",
      "Process-4 done\n",
      "Process-5 done\n",
      "CPU times: user 33min 43s, sys: 15min 55s, total: 49min 39s\n",
      "Wall time: 43min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_and_save(output_files, datasetGenerator, conversion_uvwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"out_random_sigma2k2mm\"\n",
    "batchSize = 5\n",
    "nFiles = 5 # number of output files, equal to number of processes\n",
    "\n",
    "input_files = [f'{dataPath}/{file}.root:TPCData']\n",
    "output_files = [f\"{dataPath}/data/{file}-part-{i}.tfrecord\" for i in range(nFiles)]\n",
    "datasetGenerator = io.minimal_generator(files=input_files, batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-1 started\n",
      "Process-2 started\n",
      "Process-3 started\n",
      "Process-4 started\n",
      "Process-5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:48:17.698296: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:48:17.849620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:48:17.951207: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:48:18.052097: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:48:18.223847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 100 batches\n",
      "read 200 batches\n",
      "read 300 batches\n",
      "read 400 batches\n",
      "Process-1 done\n",
      "Process-2 done\n",
      "Process-3 done\n",
      "Process-4 done\n",
      "Process-5 done\n",
      "CPU times: user 34.7 s, sys: 18.1 s, total: 52.7 s\n",
      "Wall time: 44.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_and_save(output_files, datasetGenerator, conversion_uvwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"out_random_sigma\"\n",
    "batchSize = 5\n",
    "nFiles = 5 # number of output files, equal to number of processes\n",
    "\n",
    "input_files = [f'{dataPath}/{file}.root:TPCData']\n",
    "output_files = [f\"{dataPath}/data/{file}-part-{i}.tfrecord\" for i in range(nFiles)]\n",
    "datasetGenerator = io.minimal_generator(files=input_files, batchSize=batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process-6 started\n",
      "Process-7 started\n",
      "Process-8 started\n",
      "Process-9 started\n",
      "Process-10 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-12 08:50:53.097174: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:50:53.263273: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:50:53.416226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:50:53.618560: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "2023-12-12 08:50:53.678759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: UNKNOWN ERROR (34)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read 100 batches\n",
      "read 200 batches\n",
      "Process-6 done\n",
      "Process-7 done\n",
      "Process-8 done\n",
      "Process-9 done\n",
      "Process-10 done\n",
      "CPU times: user 18.6 s, sys: 8.99 s, total: 27.6 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "process_and_save(output_files, datasetGenerator, conversion_uvwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [f\"{dataPath}/test/{file}-part-{i}.tfrecord\" for i in range(nFiles)]\n",
    "train_dataset = tf.data.TFRecordDataset(filenames, compression_type='GZIP', num_parallel_reads=5)\n",
    "# Create a description of the features.\n",
    "feature_description = {\n",
    "    'myChargeArray': tf.io.FixedLenFeature([], tf.string),\n",
    "    'target': tf.io.FixedLenFeature([], tf.string),\n",
    "\n",
    "}\n",
    "\n",
    "def _parse_function(example_proto):\n",
    "  # Parse the input `tf.train.Example` proto using the dictionary above.\n",
    "    parsed_features = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    charge, target = parsed_features['myChargeArray'], parsed_features['target']\n",
    "    # decode from bytes\n",
    "    charge = tf.io.parse_tensor(charge, tf.float64)\n",
    "    target = tf.io.parse_tensor(target, tf.float64)\n",
    "\n",
    "    return charge, target\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 73.44932381  68.96072799 199.87771658  67.77527623  51.1\n",
      "   69.17049803]\n",
      " [ 73.44932381 149.05760926 199.87771658 157.46395649  51.1\n",
      "  147.28143112]\n",
      " [ 73.44932381 146.9302146  199.87771658 156.52201359  51.1\n",
      "  144.94426642]], shape=(3, 6), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for image, target in train_dataset.take(1):\n",
    "  print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
